{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "18ytCSzY8_rxLp16o0HOjAKiJ3ifniBlR",
      "authorship_tag": "ABX9TyNImep1DEqRA9cADVCiZ4yV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamil-k/-personal-website/blob/main/Evaluating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ56wwblSHg3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcuoxnRyEOpm"
      },
      "source": [
        "#  Business Understanding/Problem Statment \n",
        "\n",
        "You have to clean this data, prepare it for ML models, build more features and ultimately build a random forest (scikit).\n",
        "Split the dataset into 80:20, train:test. \n",
        "Print the Weighted F1 on test dataset (target_satisfaction) and RMSE (Target_salary)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfR7-p0BRbC4"
      },
      "source": [
        "some of my references:  \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf\n",
        "\n",
        "https://github.com/shamil-k/Feature-Engineer-All-You-need\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/getting-deeper-into-categorical-encodings-for-machine-learning-2312acd347c8\n",
        "\n",
        "https://github.com/shamil-k/Feature-Engineer-All-You-need\n",
        "                https://heartbeat.fritz.ai/\n",
        "              \n",
        "\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\n",
        "               \n",
        "               \n",
        "https://contrib.scikit-learn.org/category_encoders/hashing.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwdtfXvntXR"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "!pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehZPo6zDjsk"
      },
      "source": [
        "\n",
        "**Reading The Data Set Using Pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLGlG5ktb3aR"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/New folder/salary,satisfaction - salary,satisfaction.csv', parse_dates=True, dayfirst=True, index_col=0)\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UgLSnKND3-0"
      },
      "source": [
        "# Data Cleaning/Preprocessing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aztw2UsLb70N"
      },
      "source": [
        "# checking the data set null values from each features\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwvo_Jv4fv0S"
      },
      "source": [
        "# Here most categorical  features having null values \n",
        "\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGPNI1JUYi7A"
      },
      "source": [
        "# Two numerical catogory having null we are fill with it's mean\n",
        "df['longitude'] = df['longitude'].fillna(df['longitude'].mean())\n",
        "df['latitude'] = df['latitude'].fillna(df['latitude'].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfyWJWiFcNYS"
      },
      "source": [
        "# Filling Missing value in Categorical feature\n",
        "\n",
        "df['FormalEducation'] = df['FormalEducation'].fillna('Not-Specified')\n",
        "df['UndergradMajor'] = df['UndergradMajor'].fillna('Not-Specified')\n",
        "df['DevType'] = df['DevType'].fillna('Not-Specified')\n",
        "df['HopeFiveYears'] = df['HopeFiveYears'].fillna('Not-Specified')\n",
        "df['YearsCoding'] = df['YearsCoding'].fillna('0-2')\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].fillna('0-2')\n",
        "df['JobSearchStatus'] = df['JobSearchStatus'].fillna('Not-Specified')\n",
        "df['LastNewJob'] = df['LastNewJob'].fillna('Not-Specified')\n",
        "df['EducationTypes'] = df['EducationTypes'].fillna('Not-Specified')\n",
        "df['AgreeDisagree1'] = df['AgreeDisagree1'].fillna('Not-Specified')\n",
        "df['Age'] = df['Age'].fillna('Not-Specified')\n",
        "df['emp_length'] = df['emp_length'].fillna('< 1 year')\n",
        "df['Age'] = df['Age'].fillna('Not-Specified')\n",
        "df['title'] = df['title'].fillna('Not-Specified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPm8BiUZEc4"
      },
      "source": [
        "# fulfilled missing values\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdOQX3fm0qIS"
      },
      "source": [
        "# removing un necessary characters \n",
        "chars_to_remove = ['years', 'or more years', 'years old', '$', '<']\n",
        "regular_expression = '[' + re.escape (''. join (chars_to_remove)) + ']'\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].str.replace(regular_expression, '', regex=True) \n",
        "df['title'] = df['title'].str.replace(regular_expression, '', regex = True)\n",
        "df['LOAN_AMT'] = df['LOAN_AMT'].str.replace(regular_expression, '', regex = True)\n",
        "df['emp_length'] = df['emp_length'].str.replace(regular_expression, '', regex = True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VZvEtcu00JC"
      },
      "source": [
        "# reseted index and removing un necessary column\n",
        "df  = df.reset_index()\n",
        "df = df.drop('id', axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnLupFCPZvj3"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_dqEZOL2HP_"
      },
      "source": [
        "# Removing un necessary characters \n",
        "chars_to_remove = ['years', 'or more years', 'years old', '$']\n",
        "regular_expression = '[' + re.escape (''. join (chars_to_remove)) + ']'\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].str.replace(regular_expression, '', regex=True) \n",
        "df['title'] = df['title'].str.replace(regular_expression, '', regex = True)\n",
        "df['LOAN_AMT'] = df['LOAN_AMT'].str.replace(regular_expression, '', regex = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_z9PXmHFrdj"
      },
      "source": [
        "## Encoding Categorical Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngtw50F26i0D"
      },
      "source": [
        "# importing libraries for encoding data\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import category_encoders as ce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFrfDH03WRv"
      },
      "source": [
        "# checking the labels in each column\n",
        "df.apply(lambda x: len(x.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KTzsO-bbSmI"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDHmFZptf5uV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqRGke18w7j"
      },
      "source": [
        "encoder = ce.TargetEncoder(cols=['Business Title'])\n",
        "encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzNzZaUOBGE7"
      },
      "source": [
        "df['Business Title'] = encoder.fit_transform(df['Business Title'],df['Target_Salary'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96l63gNJCZtT"
      },
      "source": [
        "encoder = ce.TargetEncoder(cols=['FormalEducation'])\n",
        "encoder\n",
        "df['FormalEducation'] = encoder.fit_transform(df['FormalEducation'],df['Target_Salary'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qRbK-dgIs5"
      },
      "source": [
        "**Target Guided Ordinal Encoding** \n",
        "Ordering the labels according to the target\n",
        "Replace the labels by the joint probability of being 1 or 0 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-FsDS0nglLS"
      },
      "source": [
        "df.apply(lambda x: len(x.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4NZIxDBi0E1"
      },
      "source": [
        "df.emp_length.unique()\n",
        "\n",
        "df.groupby(['emp_length'])['Target_Satisfied'].mean().sort_values().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbPTDLYaj9MP"
      },
      "source": [
        "ordinal_labels=df.groupby(['emp_length'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goZ-Z0FdkFXg"
      },
      "source": [
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "ordinal_labels2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8axK_skMe1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgyQVugJh9zg"
      },
      "source": [
        "Mean Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abs1FP3MkSdO"
      },
      "source": [
        "mean_ordinal=df.groupby(['emp_length'])['Target_Satisfied'].mean().to_dict()\n",
        "mean_ordinal\n",
        "df['emp_length']=df['emp_length'].map(mean_ordinal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-GWlGV8iypc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84ucuLFlK40"
      },
      "source": [
        "# For EducationTypes column\n",
        "ordinal_labels=df.groupby(['EducationTypes'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['EducationTypes'])['Target_Satisfied'].mean().to_dict()\n",
        "df['EducationTypes']= df['EducationTypes'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Civil Service Title column\n",
        "ordinal_labels=df.groupby(['Civil Service Title'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Civil Service Title'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Civil Service Title']= df['Civil Service Title'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Division/Work Unit Title column\n",
        "ordinal_labels=df.groupby(['Division/Work Unit'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Division/Work Unit'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Division/Work Unit']= df['Division/Work Unit'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For DevType column\n",
        "ordinal_labels=df.groupby(['DevType'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['DevType'])['Target_Satisfied'].mean().to_dict()\n",
        "df['DevType']= df['DevType'].map(mean_ordinal)\n",
        "\n",
        "# For earliest_cr_line column\n",
        "ordinal_labels=df.groupby(['earliest_cr_line'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['earliest_cr_line'])['Target_Satisfied'].mean().to_dict()\n",
        "df['earliest_cr_line']= df['earliest_cr_line'].map(mean_ordinal)\n",
        "\n",
        "# For HopeFiveYears column\n",
        "ordinal_labels=df.groupby(['HopeFiveYears'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['HopeFiveYears'])['Target_Satisfied'].mean().to_dict()\n",
        "df['HopeFiveYears']= df['HopeFiveYears'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Age column\n",
        "ordinal_labels=df.groupby(['Age'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Age'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Age']= df['Age'].map(mean_ordinal)\n",
        "\n",
        "# For purpose column\n",
        "ordinal_labels=df.groupby(['purpose'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['purpose'])['Target_Satisfied'].mean().to_dict()\n",
        "df['purpose']= df['purpose'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For Title column\n",
        "ordinal_labels=df.groupby(['title'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['title'])['Target_Satisfied'].mean().to_dict()\n",
        "df['title']= df['title'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "# For last_pymnt_d column\n",
        "ordinal_labels=df.groupby(['last_pymnt_d'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['last_pymnt_d'])['Target_Satisfied'].mean().to_dict()\n",
        "df['last_pymnt_d']= df['last_pymnt_d'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "# For last_credit_pull_d column\n",
        "ordinal_labels=df.groupby(['last_credit_pull_d'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['last_credit_pull_d'])['Target_Satisfied'].mean().to_dict()\n",
        "df['last_credit_pull_d']= df['last_credit_pull_d'].map(mean_ordinal)\n",
        "\n",
        "# For YearsCoding column\n",
        "ordinal_labels=df.groupby(['YearsCoding'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['YearsCoding'])['Target_Satisfied'].mean().to_dict()\n",
        "df['YearsCoding']= df['YearsCoding'].map(mean_ordinal)\n",
        "\n",
        "# For YearsCoding column\n",
        "ordinal_labels=df.groupby(['dateAdded'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['dateAdded'])['Target_Satisfied'].mean().to_dict()\n",
        "df['dateAdded']= df['dateAdded'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For LOAN_AMT column\n",
        "ordinal_labels=df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().to_dict()\n",
        "df['LOAN_AMT']= df['LOAN_AMT'].map(mean_ordinal)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SFlIna_73u_"
      },
      "source": [
        "**Binary Encoding**\n",
        "Binary encoding is a combination of Hash encoding and one-hot encoding. In this encoding scheme, the categorical feature is first converted into numerical using an ordinal encoder. Then the numbers are transformed in the binary number. After that binary value is split into different columns.Here We are using Binary Encoding for column that have less number of labels \n",
        "\n",
        "Binary encoding is a memory-efficient encoding scheme as it uses fewer features than one-hot encoding. Further, It reduces the curse of dimensionality for data with high cardinality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXTNm47Tmdub"
      },
      "source": [
        "encoder= ce.BinaryEncoder(cols=['UndergradMajor'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['UndergradMajor']) \n",
        "df2 = data_encoded\n",
        "df_row = pd.concat([df, df2], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['CompanySize'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['CompanySize']) \n",
        "df4 = data_encoded\n",
        "df_row = pd.concat([df_row, df4], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['YearsCodingProf'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['YearsCodingProf']) \n",
        "df5 = data_encoded\n",
        "df_row = pd.concat([df_row, df5], axis=1)\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['JobSearchStatus'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['JobSearchStatus']) \n",
        "df6 = data_encoded\n",
        "df_row = pd.concat([df_row, df6], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['LastNewJob'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['LastNewJob']) \n",
        "df7 = data_encoded\n",
        "df_row = pd.concat([df_row, df7], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['AgreeDisagree1'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['AgreeDisagree1']) \n",
        "df8 = data_encoded\n",
        "df_row = pd.concat([df_row, df8], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['term'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['term']) \n",
        "df9 = data_encoded\n",
        "df_row = pd.concat([df_row, df9], axis=1)\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['loan_status'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['loan_status']) \n",
        "df10 = data_encoded\n",
        "df_row = pd.concat([df_row, df10], axis=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D19yWioac--b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PenUJqV-Yhw"
      },
      "source": [
        "# Removing the catogorical features that are appalyed Binary encoders\n",
        "df_cleaned = df_row.drop(['UndergradMajor', 'CompanySize', 'YearsCodingProf',\n",
        "             'JobSearchStatus', 'LastNewJob', 'AgreeDisagree1', 'term','loan_status', 'YearsCoding', 'Time'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWWW8OGRhEBO"
      },
      "source": [
        "# # Performing the TimeAdded and LOAN Amount column\n",
        "# df_cleaned[\"dateAdded\"] = pd.to_datetime(df_cleaned[\"dateAdded\"]).dt.strftime(\"%Y%m%d\")\n",
        "# df_cleaned['dateAdded'] = pd.to_numeric(df_cleaned.dateAdded.str.replace('-',''))\n",
        "\n",
        "\n",
        "# df_cleaned['LOAN_AMT'] = df_cleaned.LOAN_AMT.replace('\\D', '', regex=True).astype(int)\n",
        "# print(df['LOAN_AMT'].dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54oP2i6PGFUv"
      },
      "source": [
        "# # Here the last_pymnt_amnt column have high -ve correlation \n",
        "# # dropping the -ve correlate feature\n",
        "# df_cleaned.drop('last_pymnt_amnt', axis=1)\n",
        "df_cleaned.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LplhSB8IEEXu"
      },
      "source": [
        "df_cleaned.corr()\n",
        "# Here the last_pymnt_amnt column have high -ve correlation \n",
        "# dropping the -ve correlate feature\n",
        "# df_cleaned.drop('last_pymnt_amnt', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYVZjZAZqzk7"
      },
      "source": [
        "## Dealing with Imbalanced Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N02yLw6NF5F8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf3jGUQUqxHe"
      },
      "source": [
        "# checking the imbalance in target feature\n",
        "target_satisfied = df[df['Target_Satisfied'] == 1]\n",
        "target_not_satisfied = df[df['Target_Satisfied'] == 0]\n",
        "\n",
        "count_classes = pd.value_counts(df['Target_Satisfied'], sort = True)\n",
        "\n",
        "count_classes.plot(kind = 'bar', rot=0)\n",
        "\n",
        "plt.title(\"Transaction Class Distribution\")\n",
        "LABELS = ['Target Satisfied', 'Target_Not_Satisfied']\n",
        "plt.xticks(range(2), LABELS)\n",
        "\n",
        "plt.xlabel(\"Class\")\n",
        "\n",
        "plt.ylabel(\"Frequency\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDUp_n5zrCHb"
      },
      "source": [
        "**OverSampling**\n",
        "\n",
        "I observed that the data is not in-balanced so we are doing over sampling instead of under sampling because the records are not huge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYJ_jd-zIF3a"
      },
      "source": [
        "# creating target  and input features\n",
        "print(target_satisfied.shape, target_not_satisfied.shape)\n",
        "X = df_cleaned.drop('Target_Satisfied', axis=1)\n",
        "y = df_cleaned['Target_Satisfied']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HvSfExWIKw_"
      },
      "source": [
        "# applying oversample on it\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "os =  RandomOverSampler(ratio=0.5)\n",
        "X_train_res, y_train_res = os.fit_sample(X, y)\n",
        "X_train_res.shape,y_train_res.shape\n",
        "\n",
        "\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import NearMiss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYw2nfLq_QO"
      },
      "source": [
        "# In this example I use SMOTETomek which is a method of imblearn. SMOTETomek is a hybrid method\n",
        "# which uses an under sampling method (Tomek) in with an over sampling method (SMOTE).\n",
        "os_us = SMOTETomek(ratio=0.5)\n",
        "X_train_res1, y_train_res1 = os_us.fit_sample(X, y)\n",
        "X_train_res1.shape,y_train_res1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpzrgBMZrrFO"
      },
      "source": [
        "from collections import Counter\n",
        "print('Original dataset shape {}'.format(Counter(y)))\n",
        "print('Resampled dataset shape {}'.format(Counter(y_train_res1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHWVe7MI56Ir"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = y_train_res1\n",
        "X = X_train_res1\n",
        "\n",
        "# Randomly splitting train-test data (80,20)\n",
        "# np.random.seed(10)\n",
        "# train_sub = df_cleaned.iloc[np.random.permutation(df_cleaned.index)[:2356],:].reset_index(drop=True)\n",
        "# valid_sub = df_cleaned.iloc[np.random.permutation(df_cleaned.index)[2356:],:].reset_index(drop=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHCAA_DaGKpT"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3ufaOxGZEa"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators = 100) \n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_JLF89iHx9h"
      },
      "source": [
        "# performing predictions on the test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# metrics are used to find accuracy or error\n",
        "from sklearn import metrics  \n",
        "print()\n",
        "  \n",
        "# using metrics module for accuracy calculation\n",
        "print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print('Weighted F1 Score of this model', f1_score(y_test,y_pred))\n",
        "print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3TsZveIN6-i"
      },
      "source": [
        "# saving the dataframe\n",
        "df_cleaned.to_csv('After_Preprocessing.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gfH4kmNBvtg"
      },
      "source": [
        "tf = pd.read_csv('/content/After_Preprocessing.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v5UFy2lDG_l"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mvHXFZDIgY"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from math import sqrt\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean_Squered_Error', mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R^2 SCORE:',r2)\n",
        "\n",
        "\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('ROOT MEAN SQUERE ERROR: ',rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XhNHOJIHFCG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}